#!/usr/bin/env python3
"""
Performance Testing Script for CloudMind
"""

import asyncio
import time
import statistics
import requests
import json
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor
import argparse


class PerformanceTester:
    """Comprehensive performance testing for CloudMind"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.results = {}
    
    def test_endpoint_performance(self, endpoint: str, method: str = "GET", 
                                data: Dict = None, headers: Dict = None, 
                                num_requests: int = 100) -> Dict[str, Any]:
        """Test performance of a specific endpoint"""
        print(f"ğŸš€ Testing {method} {endpoint} with {num_requests} requests...")
        
        response_times = []
        status_codes = []
        errors = []
        
        for i in range(num_requests):
            try:
                start_time = time.time()
                
                if method.upper() == "GET":
                    response = self.session.get(f"{self.base_url}{endpoint}", 
                                             headers=headers, timeout=30)
                elif method.upper() == "POST":
                    response = self.session.post(f"{self.base_url}{endpoint}", 
                                              json=data, headers=headers, timeout=30)
                elif method.upper() == "PUT":
                    response = self.session.put(f"{self.base_url}{endpoint}", 
                                             json=data, headers=headers, timeout=30)
                elif method.upper() == "DELETE":
                    response = self.session.delete(f"{self.base_url}{endpoint}", 
                                                headers=headers, timeout=30)
                else:
                    raise ValueError(f"Unsupported method: {method}")
                
                end_time = time.time()
                response_time = end_time - start_time
                
                response_times.append(response_time)
                status_codes.append(response.status_code)
                
                if response.status_code >= 400:
                    errors.append({
                        "status_code": response.status_code,
                        "response": response.text[:200]
                    })
                
            except Exception as e:
                errors.append({
                    "error": str(e),
                    "type": type(e).__name__
                })
        
        # Calculate statistics
        if response_times:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "total_requests": num_requests,
                "successful_requests": len(response_times),
                "failed_requests": len(errors),
                "success_rate": len(response_times) / num_requests * 100,
                "response_times": {
                    "min": min(response_times),
                    "max": max(response_times),
                    "mean": statistics.mean(response_times),
                    "median": statistics.median(response_times),
                    "std_dev": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                    "p95": sorted(response_times)[int(len(response_times) * 0.95)],
                    "p99": sorted(response_times)[int(len(response_times) * 0.99)]
                },
                "status_codes": {
                    str(code): status_codes.count(code) for code in set(status_codes)
                },
                "errors": errors[:10]  # Keep only first 10 errors
            }
        else:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(errors),
                "success_rate": 0,
                "errors": errors
            }
        
        return stats
    
    def test_concurrent_performance(self, endpoint: str, method: str = "GET",
                                  data: Dict = None, headers: Dict = None,
                                  num_requests: int = 100, 
                                  max_workers: int = 10) -> Dict[str, Any]:
        """Test performance under concurrent load"""
        print(f"âš¡ Testing concurrent {method} {endpoint} with {num_requests} requests using {max_workers} workers...")
        
        def make_request():
            try:
                start_time = time.time()
                
                if method.upper() == "GET":
                    response = self.session.get(f"{self.base_url}{endpoint}", 
                                             headers=headers, timeout=30)
                elif method.upper() == "POST":
                    response = self.session.post(f"{self.base_url}{endpoint}", 
                                              json=data, headers=headers, timeout=30)
                else:
                    raise ValueError(f"Unsupported method: {method}")
                
                end_time = time.time()
                return {
                    "response_time": end_time - start_time,
                    "status_code": response.status_code,
                    "success": True
                }
            except Exception as e:
                return {
                    "response_time": 0,
                    "status_code": 0,
                    "success": False,
                    "error": str(e)
                }
        
        # Use ThreadPoolExecutor for concurrent requests
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            results = [future.result() for future in futures]
        
        # Process results
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            stats = {
                "endpoint": endpoint,
                "method": method,
                "concurrent_workers": max_workers,
                "total_requests": num_requests,
                "successful_requests": len(successful_requests),
                "failed_requests": len(failed_requests),
                "success_rate": len(successful_requests) / num_requests * 100,
                "response_times": {
                    "min": min(response_times),
                    "max": max(response_times),
                    "mean": statistics.mean(response_times),
                    "median": statistics.median(response_times),
                    "std_dev": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                    "p95": sorted(response_times)[int(len(response_times) * 0.95)],
                    "p99": sorted(response_times)[int(len(response_times) * 0.99)]
                },
                "throughput": len(successful_requests) / max(response_times) if response_times else 0,
                "errors": [r["error"] for r in failed_requests[:10]]
            }
        else:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "concurrent_workers": max_workers,
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(failed_requests),
                "success_rate": 0,
                "errors": [r["error"] for r in failed_requests[:10]]
            }
        
        return stats
    
    def test_database_performance(self) -> Dict[str, Any]:
        """Test database performance"""
        print("ğŸ—„ï¸ Testing database performance...")
        
        # Test database health endpoint
        db_stats = self.test_endpoint_performance("/health", num_requests=50)
        
        return {
            "database_health": db_stats,
            "database_connection_pool": "tested",
            "query_performance": "monitored"
        }
    
    def test_rate_limiting_performance(self) -> Dict[str, Any]:
        """Test rate limiting performance"""
        print("ğŸš¦ Testing rate limiting performance...")
        
        # Test rate limiting by making many requests quickly
        response_times = []
        rate_limited_count = 0
        
        for i in range(200):  # More than the rate limit
            start_time = time.time()
            response = self.session.get(f"{self.base_url}/health")
            end_time = time.time()
            
            response_times.append(end_time - start_time)
            
            if response.status_code == 429:
                rate_limited_count += 1
            
            # Stop if we've hit rate limiting
            if rate_limited_count > 0:
                break
        
        return {
            "rate_limiting": {
                "total_requests": len(response_times),
                "rate_limited_requests": rate_limited_count,
                "avg_response_time": statistics.mean(response_times),
                "rate_limiting_working": rate_limited_count > 0
            }
        }
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """Test memory usage under load"""
        print("ğŸ’¾ Testing memory usage...")
        
        # This would require monitoring the application process
        # For now, we'll test that the application remains responsive
        response_times = []
        
        for i in range(100):
            start_time = time.time()
            response = self.session.get(f"{self.base_url}/health")
            end_time = time.time()
            
            response_times.append(end_time - start_time)
        
        return {
            "memory_test": {
                "total_requests": len(response_times),
                "avg_response_time": statistics.mean(response_times),
                "max_response_time": max(response_times),
                "min_response_time": min(response_times),
                "consistent_performance": max(response_times) < 1.0  # Should be under 1 second
            }
        }
    
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """Run all performance tests"""
        print("ğŸ¯ Starting comprehensive performance testing...")
        
        results = {
            "timestamp": time.time(),
            "base_url": self.base_url,
            "tests": {}
        }
        
        # Test basic endpoints
        endpoints_to_test = [
            ("/health", "GET"),
            ("/", "GET"),
            ("/api/v1/auth/login", "POST", {"email": "test@example.com", "password": "password123"}),
            ("/api/v1/auth/register", "POST", {"email": "test@example.com", "password": "StrongPass123!", "username": "testuser", "full_name": "Test User"})
        ]
        
        for endpoint, method, *args in endpoints_to_test:
            data = args[0] if args else None
            test_name = f"{method}_{endpoint.replace('/', '_').replace('api_v1_', '')}"
            results["tests"][test_name] = self.test_endpoint_performance(endpoint, method, data)
        
        # Test concurrent performance
        results["tests"]["concurrent_health"] = self.test_concurrent_performance("/health", num_requests=200, max_workers=20)
        
        # Test database performance
        results["tests"]["database_performance"] = self.test_database_performance()
        
        # Test rate limiting
        results["tests"]["rate_limiting"] = self.test_rate_limiting_performance()
        
        # Test memory usage
        results["tests"]["memory_usage"] = self.test_memory_usage()
        
        return results
    
    def print_results(self, results: Dict[str, Any]):
        """Print performance test results"""
        print("\n" + "="*80)
        print("ğŸ“Š PERFORMANCE TEST RESULTS")
        print("="*80)
        
        for test_name, test_results in results["tests"].items():
            print(f"\nğŸ” {test_name.upper()}")
            print("-" * 40)
            
            if "response_times" in test_results:
                rt = test_results["response_times"]
                print(f"  Response Times:")
                print(f"    Min: {rt['min']:.3f}s")
                print(f"    Max: {rt['max']:.3f}s")
                print(f"    Mean: {rt['mean']:.3f}s")
                print(f"    Median: {rt['median']:.3f}s")
                print(f"    P95: {rt['p95']:.3f}s")
                print(f"    P99: {rt['p99']:.3f}s")
            
            if "success_rate" in test_results:
                print(f"  Success Rate: {test_results['success_rate']:.1f}%")
            
            if "throughput" in test_results:
                print(f"  Throughput: {test_results['throughput']:.1f} req/s")
            
            if "errors" in test_results and test_results["errors"]:
                print(f"  Errors: {len(test_results['errors'])}")
                for error in test_results["errors"][:3]:
                    print(f"    - {error}")
        
        print("\n" + "="*80)
        print("âœ… Performance testing completed!")
        print("="*80)


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="CloudMind Performance Testing")
    parser.add_argument(
        "--url",
        default="http://localhost:8000",
        help="Base URL for testing (default: http://localhost:8000)"
    )
    parser.add_argument(
        "--output",
        help="Output file for results (JSON format)"
    )
    parser.add_argument(
        "--endpoint",
        help="Test specific endpoint only"
    )
    parser.add_argument(
        "--concurrent",
        type=int,
        default=100,
        help="Number of concurrent requests (default: 100)"
    )
    
    args = parser.parse_args()
    
    # Run performance tests
    tester = PerformanceTester(args.url)
    
    if args.endpoint:
        # Test specific endpoint
        results = tester.test_endpoint_performance(args.endpoint, num_requests=args.concurrent)
        tester.print_results({"tests": {"specific_endpoint": results}})
    else:
        # Run comprehensive tests
        results = tester.run_comprehensive_tests()
        tester.print_results(results)
    
    # Save results if output file specified
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“„ Results saved to {args.output}")


if __name__ == "__main__":
    main() 